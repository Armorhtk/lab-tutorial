{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resistant-contrary",
   "metadata": {},
   "source": [
    "#### ***教程创建时间：2021年10月9日15:22:48***\n",
    "\n",
    "#### ***教程撰写人：@Armor (胡庭恺)***\n",
    "\n",
    "#### ***教程使用说明：仅供院实验室人员交流讨论使用***\n",
    "\n",
    "\n",
    "基础内容来源：[掘金-NLP预处理技术](https://juejin.cn/post/6844903997233430536#heading-0)\n",
    "\n",
    "笔者延续其框架并根据自身学习扩充了对应的内容和实践应用\n",
    "\n",
    "![avatar](./NLP建模流程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-conservation",
   "metadata": {},
   "source": [
    "# 1.语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-swaziland",
   "metadata": {},
   "source": [
    "1. 维基百科的语料库\n",
    "2. wordNet、HowNet\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-lebanon",
   "metadata": {},
   "source": [
    "对于NLP任务来说，没有大量高质量的语料，就是巧妇难为无米之炊，是无法工作的。\n",
    "\n",
    "而获取语料的途径有很多种，最常见的方式就是直接下载开源的语料库，如：[维基百科的语料库](https://link.juejin.cn/?target=https%3A%2F%2Fdumps.wikimedia.org%2Fzhwiki%2F)。\n",
    "\n",
    "但这样开源的语料库一般都无法满足业务的个性化需要，所以就需要自己动手开发爬虫去抓取特定的内容，这也是一种获取语料库的途径。\n",
    "\n",
    "当然，每家互联网公司根据自身的业务，也都会有大量的语料数据，如：用户评论、电子书、商品描述等等，都是很好的语料库。\n",
    "\n",
    "现在，数据对于互联网公司来说就是石油，其中蕴含着巨大的商业价值。所以，小伙伴们在日常工作中一定要养成收集数据的习惯，遇到好的语料库一定要记得备份（当然是在合理合法的条件下），它将会对你解决问题提供巨大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-delaware",
   "metadata": {},
   "source": [
    "# 2.文本清洗\n",
    "\n",
    "我们通过不同的途径获取到了想要的语料库之后，接下来就需要对其进行清洗。因为很多的语料数据是无法直接使用的，其中包含了大量的无用符号、特殊的文本结构。\n",
    "\n",
    "数据类型分为：\n",
    "- 结构化数据：关系型数据、json等\n",
    "- 半结构化数据：XML、HTML等\n",
    "- 非结构化数据：Word、PDF、文本、日志等\n",
    "\n",
    "需要将原始的语料数据转化成易于处理的格式，一般在处理HTML、XML时，会使用Python的lxml库，功能非常丰富且易于使用。对一些日志或者纯文本的数据，我们可以使用正则表达式进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "coated-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "爱\n",
      "自\n",
      "然\n",
      "语\n",
      "言\n",
      "处\n",
      "理\n",
      "我爱自然语言处理\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我爱自然语言处理'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "chinese_blk = \"\"\n",
    "# 定义中文字符的正则表达式\n",
    "re_han_default = re.compile(\"([\\u4E00-\\u9FD5]+)\", re.U)\n",
    "sentence = \"我/@爱/自/然/%语/言/处/理*%\"\n",
    "# 根据正则表达式进行切分\n",
    "blocks = re_han_default.split(sentence)\n",
    "for blk in blocks:\n",
    "    # 校验单个字符是否符合正则表达式\n",
    "    if blk and re_han_default.match(blk):\n",
    "        chinese_blk += blk\n",
    "        print(blk)\n",
    "print(chinese_blk)\n",
    "# \"\".join([blk for blk in blocks if blk and re_han_default.match(blk)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-gauge",
   "metadata": {},
   "source": [
    "除了上述的内容之外，我们还需要注意中文的编码问题，在**windows平台**下中文的默认编码是**GBK（gb2312）**，而在**linux平台**下中文的默认编码是**UTF-8**。在执行NLP任务之前，我们需要统一不同来源语料的编码，避免各种莫名其妙的问题。\n",
    "\n",
    "如果大家事前无法判断语料的编码，那么我推荐大家可以使用Python的[chardet](https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fchardet%2Fchardet)库来检测编码，简单易用。既支持命令行：`chardetect somefile`，也支持代码开发。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biological-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "irish-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "rawdata = urllib.request.urlopen('http://www.baidu.com/').read()\n",
    "\n",
    "import chardet\n",
    "chardet.detect(rawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-reaction",
   "metadata": {},
   "source": [
    "# 3.分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-shame",
   "metadata": {},
   "source": [
    "中文分词和英文分词有很大的不同，英文是使用空格作为分隔符，所以英文分词基本没有什么难度。而中文是字与字直接连接，中间没有任何的分隔符，但中文是以“词”作为基本的语义单位，很多NLP任务的输入和输出都是“词”，所以中文分词的难度要远大于英文分词。\n",
    "\n",
    "中文分词是一个比较大的课题，相关的知识点和技术栈非常丰富，可以说搞懂了中文分词就等于搞懂了大半个NLP。中文分词经历了20多年的发展，克服了重重困难，取得了巨大的进步，大体可以划分成两个阶段，如下图所示：\n",
    "![avatar](./分词技术.png)\n",
    "\n",
    "目前，主流的中文分词技术采用的都是基于词典最大概率路径+未登录词识别（HMM）的方案，其中典型的代表就是jieba分词，一个热门的多语言中文分词包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "designed-hardwood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('我爱自然语言处理', '我/爱/自然语言/处理')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "sentence = \"我爱自然语言处理\"\n",
    "sentence , \"/\".join(jieba.cut(sentence))\n",
    "# jieba.cut(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-relative",
   "metadata": {},
   "source": [
    "除了 jieba分词外,还有大量的自然语言工具库也可以进行分词\n",
    "\n",
    "- [NLTK 鼻祖](https://link.zhihu.com/?target=https%3A//github.com/nltk/nltk)\n",
    "- [Hanlp分词器](https://link.zhihu.com/?target=https%3A//github.com/hankcs/HanLP)\n",
    "- [哈工大的LTP](https://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp)\n",
    "- [THULAC（THU Lexical Analyzer for Chinese）](https://link.zhihu.com/?target=https%3A//github.com/thunlp/THULAC) 由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包\n",
    "- [北京大学的pkuseg](https://link.zhihu.com/?target=https%3A//github.com/lancopku/PKUSeg-python)\n",
    "- [斯坦福分词器](https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/software/segmenter.shtml)\n",
    "- [SnowNLP](https://link.zhihu.com/?target=https%3A//github.com/isnowfy/snownlp)\n",
    "- [FudanNLP](https://link.zhihu.com/?target=https%3A//github.com/FudanNLP/fnlp)\n",
    "\n",
    "对分词感兴趣可以自行搜索相关文章，如：[知乎文章](https://zhuanlan.zhihu.com/p/86322679)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-dressing",
   "metadata": {},
   "source": [
    "# 4.标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-scott",
   "metadata": {},
   "source": [
    "标准化是为了给后续的处理提供一些必要的基础数据，包括：去掉**停用词、词汇表、训练数据**等等。\n",
    "\n",
    "当我们完成了分词之后，可以去掉**停用词**，如：“其中”、“况且”、“什么”等等，但这一步不是必须的，要根据实际业务进行选择，像关键词挖掘就需要去掉停用词，而像训练词向量就不需要。\n",
    "\n",
    "停用词可以使用实验室和公司出版的停用词表，也可以根据实验数据和场景自建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specialized-wells",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--',\n",
       " '?',\n",
       " '“',\n",
       " '”',\n",
       " '》',\n",
       " '－－',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a's\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " \"c's\",\n",
       " 'currently',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " \"i'd\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " 'thats',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t's\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'zero',\n",
       " 'zt',\n",
       " 'ZT',\n",
       " 'zz',\n",
       " 'ZZ',\n",
       " '一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即或',\n",
       " '即若',\n",
       " '却不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她的',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 停用词的读取\n",
    "def read_stopword(path):\n",
    "    \"\"\"\n",
    "    读取中文停用词表\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        stopword = file.readlines()\n",
    "    return [word.replace('\\n', '') for word in stopword]\n",
    "\n",
    "stopword = read_stopword(\"baidu_stopwords.txt\")\n",
    "# stopword[:10]\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原句:\t\t我真的挺喜欢”自然语言处理“的呀?尤其是情感--分类。这是一句实话》\n",
      "清洗后:\t\t真的挺喜欢自然语言情感分类。这是一句实话\n",
      "分词结果:\t真的/挺/喜欢/自然语言/情感/分类/。/这是/一句/实话\n"
     ]
    }
   ],
   "source": [
    "# 使用停用词过滤\n",
    "sentences = [\"我真的挺喜欢”自然语言处理“的呀?尤其是情感--分类。这是一句实话》\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    cut_words = [word for word in jieba.cut(sentence) if word not in stopword]\n",
    "    print(\"原句:\\t\\t{}\".format(sentence))\n",
    "    print(\"清洗后:\\t\\t{}\".format(\"\".join(cut_words)))\n",
    "    print(\"分词结果:\\t{}\".format(\"/\".join(cut_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-segment",
   "metadata": {},
   "source": [
    "**词汇表**是为语料库建立一个所有不重复词的列表，每个词对应一个索引值，并索引值不可以改变。词汇表的最大作用就是可以将词转化成一个向量，即One-Hot编码。\n",
    "假设我们有这样一个词汇表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "russian-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence：我爱自然语言处理 \n",
    "cutword:\n",
    "我\n",
    "爱\n",
    "自然\n",
    "语言\n",
    "处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-attraction",
   "metadata": {},
   "source": [
    "那么，我们就可以得到如下的One-Hot编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "introductory-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "我：  [1, 0, 0, 0, 0]\n",
    "爱：  [0, 1, 0, 0, 0]\n",
    "自然：[0, 0, 1, 0, 0]\n",
    "语言：[0, 0, 0, 1, 0]\n",
    "处理：[0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-aberdeen",
   "metadata": {},
   "source": [
    "这样我们就可以简单的将词转化成了计算机可以直接处理的数值化数据了。虽然One-Hot编码可以较好的完成部分NLP任务，但它的问题还是不少的。\n",
    "\n",
    "当词汇表的维度特别大的时候，就会导致经过One-Hot编码后的词向量非常稀疏，同时One-Hot编码也缺少词的语义信息。由于这些问题，才有了后面大名鼎鼎的Word2vec，以及Word2vec的升级版BERT。\n",
    "\n",
    "除了词汇表之外，我们在训练模型时，还需要提供训练数据。模型的学习可以大体分为两类：\n",
    "\n",
    "- 监督学习，在已知答案的标注数据集上，模型给出的预测结果尽可能接近真实答案，适合预测任务\n",
    "- 非监督学习，学习没有标注的数据，是要揭示关于数据隐藏结构的一些规律，适合描述任务\n",
    "\n",
    "根据不同的学习任务，我们需要提供不同的标准化数据。一般情况下，标注数据的获取成本非常昂贵，非监督学习虽然不需要花费这样的成本，但在实际问题的解决上，主流的方式还选择监督学习，因为效果更好。\n",
    "\n",
    "带标注的训练数据大概如下所示（情感分析的训练数据）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "__label__1 距离 川沙 公路 较近 公交 指示 蔡陆线 麻烦 建议 路线 房间 较为简单\n",
    "__label__1 商务 大床 房 房间 很大 床有 2M 宽 整体 感觉 经济 实惠 不错!\n",
    "__label__0 半夜 没 暖气 住!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-trick",
   "metadata": {},
   "source": [
    "其中每一行就是一条训练样本，**__label__0**和**__label__1**是分类信息，其余的部分就是分词后的文本数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-gothic",
   "metadata": {},
   "source": [
    "# 5.特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-individual",
   "metadata": {},
   "source": [
    "为了能够更好的训练模型，我们需要将文本的原始特征转化成具体特征，转化的方式主要有两种：统计和Embedding。\n",
    "<br/>\n",
    "> 原始特征：需要人类或者机器进行转化，如：文本、图像。<br/>具体特征：已经被人类进行整理和分析，可以直接使用，如：物体的重要、大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-creativity",
   "metadata": {},
   "source": [
    "## 5.1 统计\n",
    "<br/>\n",
    "统计的方式主要是计算词的词频（TF）和逆向文件频率（IDF）：\n",
    "\n",
    "- 词频，是指某一个给定的词在该文件中出现的频率，需要进行归一化，避免偏向长文本\n",
    "- 逆向文件频率，是一个词普遍重要性的度量，由总文件数目除以包含该词的文件数目\n",
    "\n",
    "那么，每个词都会得到一个TF-IDF值，用来衡量它的重要程度，计算公式如下：\n",
    "\n",
    "$$TF_IDF = TF \\times IDF = \\frac{n_{i,j}}{\\sum_{k}{n_{k,j}}} \\times \\frac{|D|}{\\{j:t_i \\in d_j \\}}$$\n",
    "\n",
    "其中$TF$的式子中$n_{i,j}$是该词在文件$d_{j}$中的出现次数，而分母则是在文件$d_{j}$中所有词的出现次数之和。\n",
    "\n",
    "而$IDF$的式子中$|D|$表示语料库中文件总数，$|{j:t_{i}\\in d_{j}}|$表示包含词$t_{i}$的文件数目，而$lg$是对结果做平滑处理。\n",
    "\n",
    "TF_IDF可以使用sklearn机器学习库进行快速搭建，也可以使用jieba等自然语言工具调用。\n",
    "\n",
    "参考资料：\n",
    "\n",
    "[[1] : sklearn-TfidfVectorizer彻底说清楚](https://zhuanlan.zhihu.com/p/67883024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expanded-destiny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "语料大小：2\n",
      "['词频 ， 是 指 某 一个 给定 的 词 在 该 文件 中 出现 的 频率 ， 需要 进行 归一化 ， 避免 偏向 长 文本', '逆向 文件 频率 ， 是 一个 词 普遍 重要 重要性 的 度量 ， 由 总 文件 数目 除以 包含 该词 的 文件 数目']\n",
      "\n",
      "词汇表大小：21\n",
      "['一个', '偏向', '出现', '包含', '度量', '归一化', '数目', '文件', '文本', '普遍', '给定', '词频', '该词', '进行', '逆向', '避免', '重要', '重要性', '除以', '需要', '频率']\n",
      "\n",
      "权重形状：(2, 21)\n",
      "[[1.         1.40546511 1.40546511 0.         0.         1.40546511\n",
      "  0.         1.         1.40546511 0.         1.40546511 1.40546511\n",
      "  0.         1.40546511 0.         1.40546511 0.         0.\n",
      "  0.         1.40546511 1.        ]\n",
      " [1.         0.         0.         1.40546511 1.40546511 0.\n",
      "  2.81093022 3.         0.         1.40546511 0.         0.\n",
      "  1.40546511 0.         1.40546511 0.         1.40546511 1.40546511\n",
      "  1.40546511 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 使用 sklearn的TFIDF算法进行特征提取\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "\n",
    "corpus = [\"词频，是指某一个给定的词在该文件中出现的频率，需要进行归一化，避免偏向长文本\",\n",
    "          \"逆向文件频率，是一个词普遍重要性的度量，由总文件数目除以包含该词的文件数目\"]\n",
    "corpus_list = []\n",
    "for corpu in corpus:\n",
    "    corpus_list.append(\" \".join(jieba.cut_for_search(corpu)))\n",
    "print(\"\\n语料大小：{}\\n{}\".format(len(corpus_list),corpus_list))\n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "tfidf = vectorizer.fit_transform(corpus_list)\n",
    "weight = tfidf.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(\"\\n词汇表大小：{}\\n{}\".format(len(vocab),vocab))\n",
    "print(\"\\n权重形状：{}\\n{}\".format(weight.shape,weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specified-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "语料大小：2\n",
      "['中华, 蜜蜂, 原产, 产于, 中国, ，, 是, 中国, 的, 土著, 蜂, ，, 适应, 中国, 各地, 的, 气候, 和, 蜜源, 条件, ，, 适于, 定, 地, 饲养, 且, 稳产, ，, 尤其, 是, 在, 南方, 方山, 山区, ，, 有着, 其他, 蜂, 种, 不可, 替代, 的, 地位, 。', '东方, 蜜蜂, 原产, 原产地, 产地, 在, 东方, 、, 简称, 东, 蜂, ，, 是, 蜜蜂, 属, 中型, 体, 中等, 的, 一个, 品种, ，, 分布, 于, 亚洲, 的, 中国, 、, 伊朗, 、, 日本, 、, 朝鲜, 等, 多个, 国家, 以及, 俄罗斯, 俄罗斯远东地区, 罗斯, 远东, 远东地区, 地区, 。, 该品, 品种, 个体, 耐寒, 耐寒性, 强, ，, 适应, 利用, 南方, 冬季, 蜜源, 。']\n",
      "\n",
      "关键词大小：13\n",
      "[('定地', 0.7969845001933333), ('蜂种', 0.7969845001933333), ('稳产', 0.73402039294), ('蜜源', 0.6672571569266667), ('中国', 0.605464137332), ('蜜蜂', 0.5859126575746667), ('土著', 0.5596897410693333), ('原产', 0.5447051912666667), ('替代', 0.48431540641933335), ('山区', 0.443906352848), ('气候', 0.38804999208133334), ('地位', 0.3471007006726667), ('条件', 0.32636787539600004)]\n",
      "\n",
      "关键词大小：15\n",
      "[('蜜蜂', 1.0339635133670588), ('东蜂', 0.703221617817647), ('俄罗斯远东地区', 0.695366830017647), ('品种', 0.6573889714858823), ('耐寒性', 0.623814163882353), ('蜜源', 0.5887563149352941), ('原产地', 0.579860375537647), ('伊朗', 0.4464084025188235), ('个体', 0.4187344468217647), ('朝鲜', 0.38749748617), ('亚洲', 0.34825364040352946), ('日本', 0.29447908918705884), ('利用', 0.29435016085176474), ('国家', 0.23532031718588234), ('中国', 0.17807768745058825)]\n"
     ]
    }
   ],
   "source": [
    "# jieba分词中 基于TFIDF的关键词提取\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "sentences = ['中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位。',\n",
    "           '东方蜜蜂原产地在东方、简称东蜂，是蜜蜂属中型体中等的一个品种，分布于亚洲的中国、伊朗、日本、朝鲜等多个国家以及俄罗斯远东地区。该品种个体耐寒性强，适应利用南方冬季蜜源。']\n",
    "seg_list = []\n",
    "for sentence in sentences:\n",
    "    seg_list.append(\", \".join(jieba.cut(sentence, cut_all=True)))\n",
    "print(\"\\n语料大小：{}\\n{}\".format(len(seg_list),seg_list))\n",
    "\n",
    "keywords = jieba.analyse.extract_tags(sentences[0], topK=20, withWeight=True, allowPOS=('n','nr','ns'))\n",
    "print(\"\\n关键词大小：{}\\n{}\".format(len(keywords),keywords))\n",
    "\n",
    "keywords = jieba.analyse.extract_tags(sentences[1], topK=20, withWeight=True, allowPOS=('n','nr','ns'))\n",
    "print(\"\\n关键词大小：{}\\n{}\".format(len(keywords),keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-developer",
   "metadata": {},
   "source": [
    "## 5.2 Embedding\n",
    "Embedding是将词嵌入到一个由神经网络的隐藏层权重构成的空间中，让语义相近的词在这个空间中距离也是相近的。Word2vec就是这个领域具有表达性的方法，大体的网络结构如下：\n",
    "![xxxx](./Embedding技术.jpg)\n",
    "\n",
    "输入层是经过One-Hot编码的词，隐藏层是我们想要得到的Embedding维度，而输出层是我们基于语料的预测结果。不断迭代这个网络，使得预测结果与真实结果越来越接近，直到收敛，我们就得到了词的Embedding编码，一个稠密的且包含语义信息的词向量，可以作为后续模型的输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-bailey",
   "metadata": {},
   "source": [
    "### 5.2.1 Word2vec 实践\n",
    "\n",
    "参考资料：\n",
    "部分资料版本老旧代码失效,gensim请以教程版本为准,保准代码可以run通\n",
    "\n",
    "[[1] : getting-started-with-word2vec-and-glove-in-python](https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python)\n",
    "\n",
    "[[2] : python︱gensim训练word2vec及相关函数与功能理解](https://blog.csdn.net/sinat_26917383/article/details/69803018)\n",
    "\n",
    "[[3] : gensim中word2vec使用](https://blog.csdn.net/u010700066/article/details/83070102)\n",
    "\n",
    "[[4] : gensim中word2vec使用](https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-sunday",
   "metadata": {},
   "source": [
    "#### 5.2.1.1 自建数据集创建和训练Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "supported-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim 版本： 3.8.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"gensim 版本：\",gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-peripheral",
   "metadata": {},
   "source": [
    "gensim是一款强大的自然语言处理工具，里面包括N多常见模型：\n",
    "基本的语料处理工具、LSI、LDA、HDP、DTM、DIM、TF-IDF、word2vec、paragraph2vec\n",
    "\n",
    "第一种方法：最简单的训练方法（快速）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interracial-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04899235"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最简单的训练方式 - 一键训练\n",
    "# 引入 word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 引入数据集\n",
    "sentences = ['中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位。',\n",
    "             '东方蜜蜂原产地在东方、简称东蜂，是蜜蜂属中型体中等的一个品种，分布于亚洲的中国、伊朗、日本、朝鲜等多个国家以及俄罗斯远东地区。该品种个体耐寒性强，适应利用南方冬季蜜源。']\n",
    "seg_list = []\n",
    "for sentence in sentences:\n",
    "    seg_list.append(\" \".join(jieba.cut(sentence, cut_all=True)))\n",
    "    \n",
    "# 切分词汇\n",
    "sentences = [s.split() for s in seg_list]\n",
    "\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(sentences, min_count=1,size=100)\n",
    "\"\"\"Word2Vec的参数\n",
    "min_count:在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，\n",
    "这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在 0~100 之间。\n",
    "\n",
    "size:参数主要是用来设置词向量的维度，Word2Vec 中的默认值是设置为 100 层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。\n",
    "\n",
    "workers：参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用。\n",
    "\"\"\"\n",
    "# 进行相关性比较\n",
    "model.wv.similarity('东方','中国')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-purpose",
   "metadata": {},
   "source": [
    "第二种方法：分阶段式的训练方法（灵活）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "associate-right",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04899235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 引入数据集\n",
    "sentences = ['中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位。',\n",
    "             '东方蜜蜂原产地在东方、简称东蜂，是蜜蜂属中型体中等的一个品种，分布于亚洲的中国、伊朗、日本、朝鲜等多个国家以及俄罗斯远东地区。该品种个体耐寒性强，适应利用南方冬季蜜源。']\n",
    "seg_list = []\n",
    "for sentence in sentences:\n",
    "    seg_list.append(\" \".join(jieba.cut(sentence, cut_all=True)))\n",
    "\n",
    "# 切分词汇\n",
    "sentences = [s.split() for s in seg_list]\n",
    "\n",
    "# 先启动一个空模型 an empty model\n",
    "new_model = gensim.models.Word2Vec(min_count=1) \n",
    "\n",
    "# 建立词汇表    \n",
    "new_model.build_vocab(sentences)                  \n",
    "\n",
    "# 训练word2vec模型     \n",
    "new_model.train(sentences, total_examples=new_model.corpus_count, epochs=new_model.epochs)  \n",
    "\n",
    "# 进行相关性比较\n",
    "new_model.wv.similarity('东方','中国')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-fellow",
   "metadata": {},
   "source": [
    "分阶段训练的另一个作用：增量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "amber-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增量训练\n",
    "temp_path = './text.model'\n",
    "old_model = gensim.models.Word2Vec.load(temp_path)\n",
    "# old_model = new_model\n",
    "more_sentences = [['东北','黑蜂','分布','在','中国','黑龙江省','饶河县','，'\n",
    "                   ,'它','是','在','闭锁','优越','的','自然环境','里','，','通过','自然选择','与','人工','进行','所','培育','的','中国','唯一','的','地方','优良','蜂种','。']]\n",
    "old_model.build_vocab(more_sentences, update=True)\n",
    "old_model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "# 进行相关性比较\n",
    "new_model.wv.similarity('东方','中国')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-tenant",
   "metadata": {},
   "source": [
    "#### 5.2.1.2 外部语料库导入得到的word2vec\n",
    "[text8下载地址](http://mattmahoney.net/dc/text8.zip)\n",
    "\n",
    "第一种方式：载入语料法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "economic-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部语料引入 【text8】：http://mattmahoney.net/dc/text8.zip\n",
    "sentences = word2vec.Text8Corpus('./text8')\n",
    "model = word2vec.Word2Vec(sentences, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "absent-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "if flag:\n",
    "    class Text8Corpus(object):\n",
    "        \"\"\"Iterate over sentences from the \"text8\" corpus, unzipped from http://mattmahoney.net/dc/text8.zip .\"\"\"\n",
    "        def __init__(self, fname, max_sentence_length=MAX_WORDS_IN_BATCH):\n",
    "            self.fname = fname\n",
    "            self.max_sentence_length = max_sentence_length\n",
    "\n",
    "        def __iter__(self):\n",
    "            # the entire corpus is one gigantic line -- there are no sentence marks at all\n",
    "            # so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens\n",
    "            sentence, rest = [], b''\n",
    "            with utils.smart_open(self.fname) as fin:\n",
    "                while True:\n",
    "                    text = rest + fin.read(8192)  # avoid loading the entire file (=1 line) into RAM\n",
    "                    if text == rest:  # EOF\n",
    "                        words = utils.to_unicode(text).split()\n",
    "                        sentence.extend(words)  # return the last chunk of words, too (may be shorter/longer)\n",
    "                        if sentence:\n",
    "                            yield sentence\n",
    "                        break\n",
    "                    last_token = text.rfind(b' ')  # last token may have been split in two... keep for next iteration\n",
    "                    words, rest = (utils.to_unicode(text[:last_token]).split(),\n",
    "                                   text[last_token:].strip()) if last_token >= 0 else ([], text)\n",
    "                    sentence.extend(words)\n",
    "                    while len(sentence) >= self.max_sentence_length:\n",
    "                        yield sentence[:self.max_sentence_length]\n",
    "                        sentence = sentence[self.max_sentence_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-amazon",
   "metadata": {},
   "source": [
    "第二种方法：载入模型文件法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "consistent-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此种方法需保证有vectors.npy\n",
    "model_normal = gensim.models.KeyedVectors.load('text.model')\n",
    "model_binary = gensim.models.KeyedVectors.load_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-macintosh",
   "metadata": {},
   "source": [
    "#### 5.2.1.3 word2vec的两种格式的存储和读取方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "engaged-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通保存\n",
    "model.wv.save('text.model')\n",
    "# model = Word2Vec.load('text8.model')\n",
    "model_normal = gensim.models.KeyedVectors.load('text.model')\n",
    "\n",
    "# 二进制保存\n",
    "model.wv.save_word2vec_format('text.model.bin', binary=True)\n",
    "# model = word2vec.Word2Vec.load_word2vec_format('text.model.bin', binary=True)\n",
    "model_binary = gensim.models.KeyedVectors.load_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "transparent-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model,model_normal,model_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-casting",
   "metadata": {},
   "source": [
    "#### 5.2.1.4 训练好的word2vec怎么用？ 养兵千日用兵一时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "republican-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load('Tencent_word2vec_100w.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "traditional-climate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5740725, 0.4951563, 0.674336)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相似度 比较\n",
    "model.similarity('肖战', '王一博'),model.similarity('肖战', '张艺兴'),model.similarity('王一博', '张艺兴')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prompt-guarantee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('如腾讯', 0.7549923658370972),\n",
       " ('包括腾讯', 0.7460228204727173),\n",
       " ('腾讯投资', 0.7444537281990051),\n",
       " ('网易', 0.7373286485671997),\n",
       " ('阿里巴巴', 0.7315868139266968),\n",
       " ('腾讯公司', 0.7294195294380188),\n",
       " ('京东', 0.7212144136428833),\n",
       " ('腾讯系', 0.7169018387794495),\n",
       " ('中国互联网巨头', 0.7136278748512268),\n",
       " ('今日头条', 0.7056840062141418)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相近词 排列\n",
    "model.most_similar(positive=['腾讯'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mineral-butter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tf2\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'土豆'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不相关词 识别\n",
    "model.doesnt_match(\"早饭 中饭 晚饭 土豆 夜宵 加餐\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "likely-prisoner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907245"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较两个列表的相似度\n",
    "model.n_similarity(['皇帝','国王',\"朕\",\"天子\"],['陛下'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "resistant-milan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.025195, -0.50379 ,  0.110249,  0.010516, -0.013777,  0.263613,\n",
       "        0.265035,  0.187569, -0.06873 , -0.192598,  0.078361,  0.509067,\n",
       "        0.003378,  0.018432,  0.204317,  0.08581 , -0.452046, -0.440482,\n",
       "        0.162501, -0.062344, -0.004219, -0.440384,  0.418173,  0.067721,\n",
       "       -0.214902, -0.347487, -0.071399,  0.653095, -0.086518, -0.202021,\n",
       "       -0.096647, -0.038436,  0.27138 ,  0.385164, -0.097811, -0.502543,\n",
       "        0.442186,  0.284154, -0.030796, -0.295057,  0.128029,  0.202773,\n",
       "        0.165646,  0.250468,  0.204295, -0.128535, -0.387213, -0.525726,\n",
       "        0.107094, -0.292191, -0.295716,  0.058024,  0.185273,  0.320373,\n",
       "       -0.039317, -0.157403,  0.294715,  0.349116,  0.0754  , -0.139013,\n",
       "       -0.07382 , -0.031074,  0.363408,  0.464056,  0.226678,  0.173192,\n",
       "       -0.219223, -0.168687,  0.448965, -0.39464 , -0.107235,  0.0795  ,\n",
       "        0.223482,  0.103793, -0.291421, -0.008667,  0.457876,  0.113457,\n",
       "       -0.08217 ,  0.183477,  0.164087,  0.181464,  0.294455,  0.284654,\n",
       "       -0.069248, -0.140722, -0.291138, -0.094483,  0.309842,  0.01051 ,\n",
       "        0.025792,  0.462967, -0.046008, -0.093971,  0.142358,  0.004334,\n",
       "        0.04074 , -0.07349 , -0.310343,  0.148609,  0.110351,  0.091484,\n",
       "       -0.13465 , -0.133116,  0.052904,  0.259545,  0.31033 , -0.41847 ,\n",
       "       -0.407389,  0.23649 ,  0.165728,  0.028856, -0.041169, -0.059593,\n",
       "       -0.625075, -0.428006, -0.349089, -0.091285, -0.263074, -0.377177,\n",
       "        0.021688, -0.182044, -0.004242, -0.150777, -0.008579,  0.121943,\n",
       "       -0.30062 , -0.187771,  0.29563 , -0.132684, -0.190837, -0.173552,\n",
       "        0.37283 ,  0.385652, -0.233371, -0.095628,  0.100631, -0.50847 ,\n",
       "        0.126221,  0.221015, -0.534615, -0.220565, -0.785852,  0.042449,\n",
       "        0.448323,  0.11468 ,  0.395656,  0.433543, -0.234909, -0.103015,\n",
       "       -0.168107,  0.038457, -0.49888 , -0.44122 , -0.029515,  0.022384,\n",
       "       -0.124822,  0.223506,  0.399015, -0.261285,  0.073153, -0.317371,\n",
       "       -0.055945,  0.391818,  0.420296,  0.096438, -0.053621,  0.203432,\n",
       "        0.194574, -0.081893,  0.053861, -0.390837, -0.353201,  0.596199,\n",
       "       -0.271015,  0.031269, -0.390773,  0.147092,  0.153871,  0.329821,\n",
       "        0.043656, -0.417865, -0.466215,  0.296282, -0.075328,  0.480437,\n",
       "       -0.075088,  0.391049,  0.196488,  0.310365,  0.164353,  0.050103,\n",
       "        0.16908 ,  0.199494,  0.55485 ,  0.565858,  0.214643, -0.323807,\n",
       "        0.129216, -0.310202], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到词向量\n",
    "model[\"重庆\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adjacent-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取词汇表\n",
    "model.vocab.keys()\n",
    "vocab = model.index2word[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-humanitarian",
   "metadata": {},
   "source": [
    "#### 5.2.1.5 word2vec 和 深度学习框架\n",
    "word2vec 如何与神经网络相结合呢?\n",
    "\n",
    "- tensorflow 版本\n",
    "- torch 版本\n",
    "\n",
    "参考资料：\n",
    "https://zhuanlan.zhihu.com/p/210808209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ceramic-toolbox",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'，': 1, '的': 2, '中国': 3, '、': 4, '蜜蜂': 5, '是': 6, '蜂': 7, '。': 8, '原产': 9, '适应': 10, '蜜源': 11, '在': 12, '南方': 13, '东方': 14, '品种': 15, '中华': 16, '产于': 17, '土著': 18, '各地': 19, '气候': 20, '和': 21, '条件': 22, '适于': 23, '定': 24, '地': 25, '饲养': 26, '且': 27, '稳产': 28, '尤其': 29, '方山': 30, '山区': 31, '有着': 32, '其他': 33, '种': 34, '不可': 35, '替代': 36, '地位': 37, '原产地': 38, '产地': 39, '简称': 40, '东': 41, '属': 42, '中型': 43, '体': 44, '中等': 45, '一个': 46, '分布': 47, '于': 48, '亚洲': 49, '伊朗': 50, '日本': 51, '朝鲜': 52, '等': 53, '多个': 54, '国家': 55, '以及': 56, '俄罗斯': 57, '俄罗斯远东地区': 58, '罗斯': 59, '远东': 60, '远东地区': 61, '地区': 62, '该品': 63, '个体': 64, '耐寒': 65, '耐寒性': 66, '强': 67, '利用': 68, '冬季': 69}\n",
      "(70, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((70, 200), (2, 64))"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#导入word2vec模型并进行预处理\n",
    "def w2v_model_preprocessing(content,w2v_model,embedding_dim,max_len=32):\n",
    "    # 初始化 `[word : index]` 字典\n",
    "    word2idx = {\"_PAD\": 0}  \n",
    "    # 训练数据 词汇表构建\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    vocab_size = len(tokenizer.word_index)  # 词库大小\n",
    "    print(tokenizer.word_index)\n",
    "    error_count = 0\n",
    "    # 存储所有 word2vec 中所有向量的数组，其中多一位，词向量全为 0， 用于 padding\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, w2v_model.vector_size))\n",
    "    print(embedding_matrix.shape)\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in w2v_model:\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "        else:\n",
    "            error_count += 1\n",
    "    # 训练数据 词向量截断补全(padding)\n",
    "    seq = tokenizer.texts_to_sequences(sentences)\n",
    "    trainseq = pad_sequences(seq, maxlen=max_len,padding='post')\n",
    "    return embedding_matrix,trainseq\n",
    "\n",
    "\n",
    "# 从文本 到 tf可用的word2vec\n",
    "sentences = ['中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位。',\n",
    "             '东方蜜蜂原产地在东方、简称东蜂，是蜜蜂属中型体中等的一个品种，分布于亚洲的中国、伊朗、日本、朝鲜等多个国家以及俄罗斯远东地区。该品种个体耐寒性强，适应利用南方冬季蜜源。']\n",
    "seg_list = []\n",
    "for sentence in sentences:\n",
    "    seg_list.append(\" \".join(jieba.cut(sentence, cut_all=True)))\n",
    "sentences = [s.split() for s in seg_list]\n",
    "\n",
    "# 一些超参数\n",
    "max_len = 64\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "embedding_matrix,train_data = w2v_model_preprocessing(sentences,model,embedding_dim,max_len)\n",
    "embedding_matrix.shape,train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "incomplete-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Input, Lambda, Reshape,concatenate\n",
    "from tensorflow.keras.layers import Embedding,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Flatten,BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_textcnn(max_len,embeddings_dim,embeddings_matrix):\n",
    "    #构建textCNN模型\n",
    "    main_input = Input(shape=(max_len,), dtype='float64')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(\n",
    "                         len(embeddings_matrix), #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词\n",
    "                         embeddings_dim, # 嵌入单词的向量空间的大小\n",
    "                         input_length=max_len, #规定长度 \n",
    "                         weights=[embeddings_matrix],# 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "                         trainable=False # 设置词向量不作为参数进行更新\n",
    "                         )\n",
    "    embed = embedder(main_input)\n",
    "    flat = Flatten()(embed)\n",
    "    dense01 = Dense(5096, activation='relu')(flat)\n",
    "    dense02 = Dense(1024, activation='relu')(dense01)\n",
    "    main_output = Dense(2, activation='softmax')(dense02)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "higher-melissa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 64, 200)           14000     \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 5096)              65233896  \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1024)              5219328   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 70,469,274\n",
      "Trainable params: 70,455,274\n",
      "Non-trainable params: 14,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "TextCNN = build_textcnn(64,embedding_dim,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "nominated-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.6547 - accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "X_train, y_train = train_data,to_categorical([0,1], num_classes=2)\n",
    "# 粗糙的模型训练\n",
    "history = TextCNN.fit(X_train, y_train,\n",
    "                      batch_size=2,\n",
    "                      epochs=3,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-scope",
   "metadata": {},
   "source": [
    "#### 5.2.1.6 word2vec的可视化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "adequate-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def wv_visualizer(model,word):\n",
    "\n",
    "    # 寻找出最相似的十个词\n",
    "    words = [wp[0] for wp in  model.wv.most_similar(word,topn=10)]\n",
    "    # 提取出词对应的词向量\n",
    "    wordsInVector = [model.wv[word] for word in words]\n",
    "    wordsInVector\n",
    "    # 进行 PCA 降维\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(wordsInVector)\n",
    "    X = pca.transform(wordsInVector)\n",
    "    # 绘制图形\n",
    "    xs = X[:, 0]\n",
    "    ys = X[:, 1]\n",
    "    # draw\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.scatter(xs, ys, marker = 'o')\n",
    "    for i, w in enumerate(words):\n",
    "        plt.annotate(\n",
    "            w,\n",
    "            xy = (xs[i], ys[i]), xytext = (6, 6),\n",
    "            textcoords = 'offset points', ha = 'left', va = 'top',\n",
    "            **dict(fontsize=10)\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 调用时传入目标词组即可\n",
    "wv_visualizer(model,[\"man\",\"king\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-devon",
   "metadata": {},
   "source": [
    "#### 5.2.1.7 探索实验\n",
    "\n",
    "前面看过了TFIDF + 机器学习模型, 在这里不妨思考一下word2vec和机器学习模型的组合,这有助理解模型和数据的耦合关系\n",
    "\n",
    "试这使用TFIDF + svm 和 word2vec + svm 完成一个文本分类任务，并成功预测新的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "productive-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'…………'"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"…………\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-integer",
   "metadata": {},
   "source": [
    "### 5.2.2 bert 实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-canon",
   "metadata": {},
   "source": [
    "Bert是NLP目前最流行的范式（预训练模型），这里不做深入的原理讨论\n",
    "\n",
    "目前Bert的使用已经涌现出适配各框架的代码库或源码文件\n",
    "- google 出版的 bert源码： 适配tensorflow1.x\n",
    "- hugging face 出版的 transformers： 适配torch 和 tensorflow2.x\n",
    "- 苏剑林（苏神）出版的 bert4keras：适配keras不高于2.4.2版本\n",
    "\n",
    "本教程当前只给出基于transformer的bert快速实践方法,关于bert的内容还比较单薄，关于bert的各种实践方法和技巧还需要各位同好完善与积累。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exceptional-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，像往常一样，我们导入库，然后设置随机种子\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-sampling",
   "metadata": {},
   "source": [
    "Transformer 已经用特定的词汇进行了训练，这意味着我们需要使用完全相同的词汇进行训练，并且和 Transformer 最初训练时相同的方式标记我们的数据。transformers 库为每个提供的transformer 模型都有分词器。 在这种情况下，我们使用忽略大小写的 BERT 模型（即每个单词都会小写）。 我们通过加载预训练的“bert-base-uncased”标记器来实现这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-acting",
   "metadata": {},
   "source": [
    "#### 5.2.2.1 tokenizer of Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "everyday-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 加载BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fantastic-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "['这', '世', '界', '很', '大', '?']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.vocab))\n",
    "# 使用tokenizer.tokenize方法对字符串进行分词，并统一大小写\n",
    "tokens = tokenizer.tokenize('这世界很大?')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precious-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6821, 686, 4518, 2523, 1920, 136]\n",
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corporate-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bottom-munich",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1420, 1409,  671, 2375, 6413, 1963, 1420,  671, 2375, 6413,  102,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtext = \"听君一席话如听一席话\"\n",
    "test_tokenizer = tokenizer.encode(testtext,\n",
    "                                  add_special_tokens=True,\n",
    "                                  max_length=32,\n",
    "                                  truncation=True,\n",
    "                                  padding=\"max_length\",\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "# test_tokenizer = tokenizer.encode_plus(testtext,\n",
    "#                                        add_special_tokens=True,\n",
    "#                                        max_length=32,\n",
    "#                                        truncation=True,\n",
    "#                                        padding=\"max_length\",\n",
    "#                                        return_tensors=\"pt\")\n",
    "test_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-aruba",
   "metadata": {},
   "source": [
    " 参考资料：使用 Bert 做 特征提取\n",
    "https://www.it610.com/article/1290732452595179520.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "canadian-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 768]),\n",
       " tensor([[[ 0.0378,  0.6450,  0.4189,  ...,  0.2103, -0.2012, -0.3666],\n",
       "          [ 0.2128, -0.3010, -0.5282,  ..., -0.1046, -0.1741, -0.1394],\n",
       "          [-0.1151, -0.4983, -0.3201,  ..., -0.5699, -0.3356, -0.5096],\n",
       "          ...,\n",
       "          [-0.0873,  0.1830, -0.2017,  ...,  0.2609, -0.0327,  0.1089],\n",
       "          [-0.1398,  0.1641, -0.3668,  ...,  0.1741, -0.0157,  0.1408],\n",
       "          [-0.2374,  0.1438, -0.1978,  ...,  0.2875, -0.1162,  0.2005]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Bert 做 特征提取\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('./bert')\n",
    "with torch.no_grad(): #禁用梯度计算 因为只是前向传播获取隐藏层状态，所以不需要计算梯度\n",
    "    last_hidden_states = bert(test_tokenizer)[0]\n",
    "last_hidden_states.shape,last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-reggae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n",
      "GeForce RTX 3060\n",
      "tensor([[0.0290, 0.4019, 0.2598],\n",
      "        [0.3666, 0.0583, 0.7006],\n",
      "        [0.0518, 0.4681, 0.6738]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "print(flag)\n",
    "\n",
    "ngpu= 1\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.rand(3,3).cuda()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pharmaceutical-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  tokenizer_language='cn',\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-glossary",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 微博情感分析数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaging-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119963</th>\n",
       "      <td>0</td>\n",
       "      <td>今天注定应该有个不眠夜 MMP[怒]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119964</th>\n",
       "      <td>0</td>\n",
       "      <td>希望隧道今天一直堵车，直到咪咪安全走出隧道[泪][泪][泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119965</th>\n",
       "      <td>0</td>\n",
       "      <td>今天破了纪录了，从中午11点45到晚上9点28，泡在一家咖啡厅10个小时，午饭晚饭都在这解决...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119966</th>\n",
       "      <td>0</td>\n",
       "      <td>[泪]//@高会民:[泪]//@第一微闻: [泪]//@神笔记: [泪]//@澜澜雨露: [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119967</th>\n",
       "      <td>0</td>\n",
       "      <td>[可怜]哎，何必呀何必！何苦啊何苦！ //@saskya岚:你妹的，我的人生不能如此喜剧吧？...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119968 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review\n",
       "0           1              ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]\n",
       "1           1  @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...\n",
       "2           1  姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...\n",
       "3           1                                         美~~~~~[爱你]\n",
       "4           1                                  梦想有多大，舞台就有多大![鼓掌]\n",
       "...       ...                                                ...\n",
       "119963      0                                 今天注定应该有个不眠夜 MMP[怒]\n",
       "119964      0                     希望隧道今天一直堵车，直到咪咪安全走出隧道[泪][泪][泪]\n",
       "119965      0  今天破了纪录了，从中午11点45到晚上9点28，泡在一家咖啡厅10个小时，午饭晚饭都在这解决...\n",
       "119966      0  [泪]//@高会民:[泪]//@第一微闻: [泪]//@神笔记: [泪]//@澜澜雨露: [...\n",
       "119967      0  [可怜]哎，何必呀何必！何苦啊何苦！ //@saskya岚:你妹的，我的人生不能如此喜剧吧？...\n",
       "\n",
       "[119968 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./weibo_sentiment.csv\")\n",
    "df.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outer-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchtext.data import TabularDataset \n",
    "\n",
    "train_x = TabularDataset(path = 'weibo_sentiment.csv',\n",
    "                         format = 'csv',skip_header=True,\n",
    "                         fields = [('label',LABEL),('review',TEXT)])\n",
    "\n",
    "train_data, test_data = train_x.split(split_ratio=0.1,random_state = random.seed(42))\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "consistent-difference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8399\n",
      "Number of validation examples: 3600\n",
      "Number of testing examples: 107989\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "arctic-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'review': [137, 710, 3367, 8429, 138, 7961, 2958, 140, 138, 7961, 2958, 140]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spanish-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "certain-willow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0, '1': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pressing-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key = lambda x:len(x.review),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-balance",
   "metadata": {},
   "source": [
    "#### 5.2.2.3 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blocked-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('./bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "natural-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "close-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 105,026,817 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "rubber-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = [16]\n",
    "num_layers = 2\n",
    "for i in range(1,num_layers):\n",
    "    print(hidden_dim[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demographic-delaware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,759,169 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "freelance-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-optimization",
   "metadata": {},
   "source": [
    "#### 5.2.2.4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "leading-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "temporal-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.review).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.review).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "biological-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "useful-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.315 | Train Acc: 85.55%\n",
      "\t Val. Loss: 0.176 |  Val. Acc: 93.51%\n",
      "Epoch: 02 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.101 | Train Acc: 96.61%\n",
      "\t Val. Loss: 0.087 |  Val. Acc: 97.17%\n",
      "Epoch: 03 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.80%\n",
      "\t Val. Loss: 0.054 |  Val. Acc: 97.82%\n",
      "Epoch: 04 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.14%\n",
      "\t Val. Loss: 0.068 |  Val. Acc: 97.33%\n",
      "Epoch: 05 | Epoch Time: 0m 19s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.07%\n",
      "\t Val. Loss: 0.049 |  Val. Acc: 98.28%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-completion",
   "metadata": {},
   "source": [
    "#### 5.2.2.5 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "minus-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    if result < 0.5:\n",
    "        print(\"情感极性：负面 {:.3f}\".format(result))\n",
    "        print(\"情感极性：正面 {:.3f}\".format(1-result))\n",
    "    else:\n",
    "        print(\"情感极性：正面 {:.3f}\".format(result))\n",
    "        print(\"情感极性：负面 {:.3f}\".format(1-result))\n",
    "    return prediction.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "healthy-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感极性：负面 0.247\n",
      "情感极性：正面 0.753\n"
     ]
    }
   ],
   "source": [
    "result = predict_sentiment(model, tokenizer, \"何必呀何必！何苦啊何苦！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "centered-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感极性：负面 0.247\n",
      "情感极性：正面 0.753\n"
     ]
    }
   ],
   "source": [
    "result = predict_sentiment(model, tokenizer, \"美~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-lewis",
   "metadata": {},
   "source": [
    "综上所述，本教程就将NLP预处理的部分介绍完毕了，已经涵盖了大部分的NLP内容，后续我们来聊聊NLP的一些具体业务场景。\n",
    "![xxxx](./NLP发展历程.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
